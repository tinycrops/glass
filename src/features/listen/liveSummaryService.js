require('dotenv').config();
const { BrowserWindow, ipcMain } = require('electron');
const { spawn } = require('child_process');
const { saveDebugAudio } = require('./audioUtils.js');
const { getSystemPrompt } = require('../../common/prompts/promptBuilder.js');
const { connectToOpenAiSession, createOpenAiGenerativeClient, getOpenAiGenerativeModel } = require('../../common/services/openAiClient.js');
const sqliteClient = require('../../common/services/sqliteClient'); // Import sqliteClient
const dataService = require('../../common/services/dataService'); // To get current user ID

const {isFirebaseLoggedIn,getCurrentFirebaseUser} = require('../../electron/windowManager.js');

// API í‚¤ë¥¼ ì €ì¥ëœ ê°’ì—ì„œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (Stashed changes)
function getApiKey() {
    const { getStoredApiKey } = require('../../electron/windowManager.js');
    const storedKey = getStoredApiKey();
    
    if (storedKey) {
        console.log('[LiveSummaryService] Using stored API key');
        return storedKey;
    }
    
    // í™˜ê²½ë³€ìˆ˜ fallback (Updated upstream + Stashed changes)
    const envKey = process.env.OPENAI_API_KEY
    if (envKey) {
        console.log('[LiveSummaryService] Using environment API key');
        return envKey;
    }
    
    console.error('[LiveSummaryService] No API key found in storage or environment');
    return null;
}

// Conversation tracking variables
let currentSessionId = null; // This will now be the DB session ID
let conversationHistory = []; // This can be removed or used for short-term prompt building
let isInitializingSession = false;

// STT (Speech-to-Text) WebSocket ì„¸ì…˜
let mySttSession = null;
let theirSttSession = null;
let myCurrentUtterance = '';
let theirCurrentUtterance = '';

// ë°±ì—… ë©”ì»¤ë‹ˆì¦˜: turnComplete ì´ë²¤íŠ¸ê°€ ì˜¤ì§€ ì•Šì„ ë•Œ ëŒ€ë¹„
let myLastPartialText = '';
let theirLastPartialText = '';
let myInactivityTimer = null;
let theirInactivityTimer = null;
const INACTIVITY_TIMEOUT = 3000; // 3ì´ˆ ë™ì•ˆ ìƒˆë¡œìš´ ìŒì„±ì´ ì—†ìœ¼ë©´ ì™„ë£Œë¡œ ê°„ì£¼

// ---------------------------------------------------------------------------
// ğŸ›ï¸  Turn-completion debouncing
// ---------------------------------------------------------------------------
// Very aggressive VAD (e.g. 50 ms) tends to split one spoken sentence into
// many "completed" events.  To avoid creating a separate chat bubble for each
// of those micro-turns we debounce the *completed* events per speaker.  Any
// completions that arrive within this window are concatenated and flushed as
// **one** final turn.

const COMPLETION_DEBOUNCE_MS = 2000; // adjust as needed for UX

let myCompletionBuffer = '';
let theirCompletionBuffer = '';
let myCompletionTimer = null;
let theirCompletionTimer = null;

function flushMyCompletion() {
    if (!myCompletionBuffer.trim()) return;

    const finalText = myCompletionBuffer.trim();
    // Save to DB & send to renderer as final
    saveConversationTurn('Me', finalText);
    sendToRenderer('stt-update', {
        speaker: 'Me',
        text: finalText,
        isPartial: false,
        isFinal: true,
        timestamp: Date.now(),
    });

    myCompletionBuffer = '';
    myCompletionTimer = null;
    myCurrentUtterance = ''; // Reset utterance accumulator on flush
    sendToRenderer('update-status', 'Listening...');
}

function flushTheirCompletion() {
    if (!theirCompletionBuffer.trim()) return;

    const finalText = theirCompletionBuffer.trim();
    saveConversationTurn('Them', finalText);
    sendToRenderer('stt-update', {
        speaker: 'Them',
        text: finalText,
        isPartial: false,
        isFinal: true,
        timestamp: Date.now(),
    });

    theirCompletionBuffer = '';
    theirCompletionTimer = null;
    theirCurrentUtterance = ''; // Reset utterance accumulator on flush
    sendToRenderer('update-status', 'Listening...');
}

function debounceMyCompletion(text) {
    // Append with space if needed
    myCompletionBuffer += (myCompletionBuffer ? ' ' : '') + text;

    if (myCompletionTimer) clearTimeout(myCompletionTimer);
    myCompletionTimer = setTimeout(flushMyCompletion, COMPLETION_DEBOUNCE_MS);
}

function debounceTheirCompletion(text) {
    theirCompletionBuffer += (theirCompletionBuffer ? ' ' : '') + text;

    if (theirCompletionTimer) clearTimeout(theirCompletionTimer);
    theirCompletionTimer = setTimeout(flushTheirCompletion, COMPLETION_DEBOUNCE_MS);
}

// Audio capture
let systemAudioProc = null;

let analysisIntervalId = null;

/**
 * ëŒ€í™” ê¸°ë¡ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚¤ê¸° ìœ„í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
 * @param {Array<string>} conversationTexts - ëŒ€í™” í…ìŠ¤íŠ¸ ë°°ì—´ ["me: ~~~", "them: ~~~", ...]
 * @param {number} maxTurns - í¬í•¨í•  ìµœê·¼ í„´ì˜ ìµœëŒ€ ê°œìˆ˜
 * @returns {string} - í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·ëœ ëŒ€í™” ë¬¸ìì—´
 */
function formatConversationForPrompt(conversationTexts, maxTurns = 30) {
    if (conversationTexts.length === 0) return '';
    return conversationTexts
        .slice(-maxTurns)
        .join('\n');
}

async function makeOutlineAndRequests(conversationTexts, maxTurns = 30) {
    console.log(`ğŸ” makeOutlineAndRequests called - conversationTexts: ${conversationTexts.length}`);
    

    if (conversationTexts.length === 0) {
        console.log('âš ï¸ No conversation texts available for analysis');
        return null;
    }

    const recentConversation = formatConversationForPrompt(conversationTexts, maxTurns);
    console.log(`ğŸ“ Recent conversation (${conversationTexts.length} texts):\n${recentConversation.substring(0, 200)}...`);
    
    // Build system prompt with conversation history directly embedded
    // const basePrompt = getSystemPrompt('cluely_analysis', '', false);
    const basePrompt = getSystemPrompt('cluely_analysis_latest', '', false);
    const systemPrompt = basePrompt.replace('{{CONVERSATION_HISTORY}}', recentConversation);
    console.log(`ğŸ“‹ Generated system prompt with conversation history`);

    try {
        // OpenAI API í˜•ì‹ìœ¼ë¡œ messages ë°°ì—´ êµ¬ì„±
        const messages = [
            {
                role: 'system',
                content: systemPrompt
            },
            {
                role: 'user',
                content: 'Analyze the conversation and provide a summary with key topics and suggested questions.'
            }
        ];
        
        console.log('ğŸ¤– Sending analysis request to OpenAI...');
        
        // OpenAI API í˜¸ì¶œ
        const API_KEY = getApiKey();
        if (!API_KEY) {
            throw new Error('No API key available');
        }
        const loggedIn = isFirebaseLoggedIn();          // true âœ vKey, false âœ apiKey
        const keyType  = loggedIn ? 'vKey' : 'apiKey';
        console.log(`[LiveSummary] keyType: ${keyType}`);
        
        const fetchUrl = keyType === 'apiKey'
            ? 'https://api.openai.com/v1/chat/completions'
            : 'https://api.portkey.ai/v1/chat/completions';
    
        const headers  = keyType === 'apiKey'
            ? {                                           // â‘  ì¼ë°˜ OpenAI Key
                    'Authorization': `Bearer ${API_KEY}`,
                    'Content-Type' : 'application/json',
                }
            : {                                           // â‘¡ Portkey vKey
                    'x-portkey-api-key'   : 'gRv2UGRMq6GGLJ8aVEB4e7adIewu',
                    'x-portkey-virtual-key': API_KEY,
                    'Content-Type'        : 'application/json',
                };

        const response = await fetch(fetchUrl, {
                method : 'POST',
                headers,
                body   : JSON.stringify({
                    model       : 'gpt-4.1',
                    messages,
                    temperature : 0.7,
                    max_tokens  : 1024
                })
            });
        
        if (!response.ok) {
            throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
        }
        
        const result = await response.json();
        const responseText = result.choices[0].message.content.trim();
        console.log(`âœ… Analysis response received: ${responseText}`);

        // const parsedData = parseResponseText(responseText);
        const structuredData = parseResponseText(responseText);
        
        // --- NEW: Save AI message and Summary to DB ---
        if (currentSessionId) {
            // Save the user's implicit request
            await sqliteClient.addAiMessage({
                sessionId: currentSessionId,
                role: 'user',
                content: 'Analyze the conversation and provide a summary...' // Abridged
            });

            // Save the AI's response
            await sqliteClient.addAiMessage({
                sessionId: currentSessionId,
                role: 'assistant',
                content: responseText
            });

            // Save the parsed summary
            await sqliteClient.saveSummary({
                sessionId: currentSessionId,
                tldr: structuredData.topic.header || 'Summary not available.',
                text: responseText,
                bullet_json: JSON.stringify(structuredData.summary),
                action_json: JSON.stringify(structuredData.actions)
            });
            console.log(`[DB] Saved AI analysis and summary for session ${currentSessionId}`);
        }
        // --- END NEW ---

        // return parsedData;

        
        return structuredData;  // ì§ì ‘ structuredData ë°˜í™˜
        
        
    } catch (error) {
        console.error('âŒ Error during analysis generation:', error.message);
        console.error('Full error details:', error);
        if (error.response) {
            console.error('API response error:', error.response);
        }
        return null;
    }
}


/**
 * AIì˜ ë¶„ì„ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ìš”ì•½, ì£¼ìš” í† í”½ ë° ì‹¤í–‰ í•­ëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
 * ì´ ë²„ì „ì€ ë¨¼ì € ëª¨ë“  êµµì€ í—¤ë”ë¥¼ ì°¾ì•„ í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆˆ ë‹¤ìŒ, ê° ì„¹ì…˜ì˜ ë‚´ìš©ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
 * summary í•­ëª©ì—ì„œëŠ” í—¤ë” íƒœê·¸(**...:**)ê°€ ì œê±°ë©ë‹ˆë‹¤.
 *
 * @param {string} responseText - AIì˜ ì›ì‹œ í…ìŠ¤íŠ¸ ì‘ë‹µ.
 * @returns {{summary: string[], topic: {header: string, bullets: string[]}, actions: string[]}} - êµ¬ì¡°í™”ëœ ë°ì´í„°.
 */
function parseResponseText(responseText) {
    const structuredData = {
        summary: [],
        topic: { header: '', bullets: [] },
        actions: [],
        followUps: []
    };

    try {
        const lines = responseText.split('\n');
        const sections = [];
        // ì²« í—¤ë”ê°€ ë‚˜ì˜¤ê¸° ì „ì˜ ë‚´ìš©ì„ ë‹´ì„ ì´ˆê¸° ì„¹ì…˜
        let currentSection = { header: 'Introduction', content: [] };

        // 1. ëª¨ë“  ë©”ì¸ í—¤ë”ë¥¼ ì°¾ì•„ í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        for (const line of lines) {
            // ì¤„ì˜ ì‹œì‘ì´ "**...**" íŒ¨í„´ì¸ ê²½ìš° ë©”ì¸ í—¤ë”ë¡œ ê°„ì£¼
            const headerMatch = line.trim().match(/^\*\*(.*)\*\*$/);
            
            // ë‹¨, ë¶ˆë¦¿ í¬ì¸íŠ¸('-')ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ëŠ” í•˜ìœ„ í•­ëª©ì´ë¯€ë¡œ ì œì™¸
            if (headerMatch && !line.trim().startsWith('-')) {
                // ì´ì „ê¹Œì§€ ìˆ˜ì§‘ëœ ì„¹ì…˜ì„ ë°°ì—´ì— ì¶”ê°€
                if (currentSection.header || currentSection.content.length > 0) {
                    sections.push(currentSection);
                }
                // ìƒˆ ì„¹ì…˜ ì‹œì‘
                currentSection = { header: headerMatch[1].trim(), content: [] };
            } else {
                // í˜„ì¬ ì„¹ì…˜ì˜ ë‚´ìš©ìœ¼ë¡œ ì¶”ê°€
                currentSection.content.push(line);
            }
        }
        sections.push(currentSection); // ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€

        // 2. ì‹ë³„ëœ ê° ì„¹ì…˜ì„ ìˆœíšŒí•˜ë©° íŒŒì‹±í•©ë‹ˆë‹¤.
        for (const section of sections) {
            const headerText = section.header.toLowerCase().replace(/:$/, '').trim();
            const contentText = section.content.join('\n');

            // 2a. 'ìš”ì•½' ë° 'í† í”½' ì„¹ì…˜ ì²˜ë¦¬
            const summaryKeywords = ['summary', 'key', 'topic', 'main', 'point', 'overview', 'headline'];
            if (summaryKeywords.some(k => headerText.includes(k)) || headerText === 'introduction') {
                // ë¶ˆë¦¿ í¬ì¸íŠ¸ '**í—¤ë”:** ë‚´ìš©' í˜•ì‹ì˜ ëª¨ë“  í•­ëª©ì„ ì°¾ìŒ
                const pointRegex = /^\s*[-\*]\s*\*\*(?<header>[^:]+):\*\*(?<description>(?:.|\n(?!\s*[-\*]))*)/gm;
                const allPoints = [...contentText.matchAll(pointRegex)];

                for (const match of allPoints) {
                    const { header, description } = match.groups;
                    
                    // ì²« ë²ˆì§¸ ì£¼ìš” í¬ì¸íŠ¸ë¥¼ 'topic'ìœ¼ë¡œ ì„¤ì •
                    if (!structuredData.topic.header) {
                        structuredData.topic.header = `${header.trim()}:`;
                        console.log('ğŸ“Œ Found main topic header:', structuredData.topic.header);
                        if (description.trim()) {
                            const topicBullets = description.trim().split('\n').map(l => l.trim()).filter(Boolean);
                            structuredData.topic.bullets.push(...topicBullets);
                            topicBullets.forEach(b => console.log('ğŸ“Œ Found topic bullet:', b));
                        }
                    } else { 
                        // âœ… ìˆ˜ì •ëœ ë¶€ë¶„: ë‚˜ë¨¸ì§€ëŠ” 'summary'ë¡œ ì¶”ê°€í•˜ë˜, í—¤ë” í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                        const summaryDescription = description.trim().replace(/\s+/g, ' ');
                        structuredData.summary.push(summaryDescription);
                        console.log('ğŸ“Œ Found summary point:', summaryDescription);
                    }
                }
            }

            // 2b. 'ì„¤ëª…' ì„¹ì…˜ ì²˜ë¦¬
            const explanationKeywords = ['extended', 'explanation'];
            if (explanationKeywords.some(k => headerText.includes(k))) {
                const sentences = contentText.trim().split(/\.\s+/)
                    .filter(s => s.trim().length > 0)
                    .map(s => s.trim() + (s.endsWith('.') ? '' : '.'));
                    
                structuredData.topic.bullets.push(...sentences.slice(0, 3));
                sentences.slice(0, 3).forEach(b => console.log('ğŸ“Œ Found explanation bullet:', b));
            }

           

            // 2c. 'ì§ˆë¬¸' ì„¹ì…˜ ì²˜ë¦¬
            const questionKeywords = ['suggest', 'follow-up', 'question'];
            if (questionKeywords.some(k => headerText.includes(k))) {
                const questionLines = contentText.split('\n')
                    .map(line => line.replace(/^\s*(\d+\.|-|\*)\s*/, '').trim())
                    .filter(line => line.includes('?') && line.length > 10);
                
                structuredData.actions.push(...questionLines.slice(0, 3));
                questionLines.slice(0, 3).forEach(q => console.log('ğŸ“Œ Found question:', q));
            }
        }

        // 3. ìµœì¢… ì •ë¦¬ ë° ê¸°ë³¸ê°’ ì„¤ì •
        // ê³ ì • ì•¡ì…˜ ì¶”ê°€ ë° ì¤‘ë³µ ì œê±°
        const fixedActions = ["What should i say next?", "Suggest follow-up questions"];
        structuredData.actions = [...new Set([...structuredData.actions, ...fixedActions])];

        // ë°°ì—´ í¬ê¸° ì œí•œ
        structuredData.summary = structuredData.summary.slice(0, 5);
        structuredData.topic.bullets = [...new Set(structuredData.topic.bullets)].slice(0, 3);
        structuredData.actions = structuredData.actions.slice(0, 5);
        structuredData.followUps = [
            "Draft a follow-up email",
            "Generate action items", 
            "Show summary"
        ];

    } catch (error) {
        console.error('âŒ Error parsing response text:', error);
        // ì—ëŸ¬ ë°œìƒ ì‹œ ì•ˆì „í•œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            summary: [],
            topic: { header: '', bullets: [] },
            actions: ["What should i say next?", "Suggest follow-up questions"],
            followUps: [
                "Draft a follow-up email",
                "Generate action items",
                "Show summary"
            ]
        };
    }
    
    console.log('ğŸ“Š Final structured data:', JSON.stringify(structuredData, null, 2));
    return structuredData;
}

/**
 * ëŒ€í™” í…ìŠ¤íŠ¸ê°€ 5ê°œ ì´ìƒ ìŒ“ì¼ ë•Œë§ˆë‹¤ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
 */
async function triggerAnalysisIfNeeded() {
    if (conversationHistory.length >= 5 && conversationHistory.length % 5 === 0) {
        console.log(`ğŸš€ Triggering analysis (non-blocking) - ${conversationHistory.length} conversation texts accumulated`);
        
        // awaitë¥¼ ì œê±°í•˜ì—¬ non-blockingìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
        makeOutlineAndRequests(conversationHistory).then(data => {
            if (data) {
                console.log('ğŸ“¤ Sending structured data to renderer');
                // í•˜ë‚˜ì˜ ì±„ë„ë¡œ í†µí•© ì „ì†¡
                sendToRenderer('update-structured-data', data);
            } else {
                console.log('âŒ No analysis data returned from non-blocking call');
            }
        }).catch(error => {
            console.error('âŒ Error in non-blocking analysis:', error);
        });
    }
}

/**
 * 10ì´ˆë§ˆë‹¤ ì£¼ê¸°ì ìœ¼ë¡œ ê°œìš” ë° ë¶„ì„ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤ì¼€ì¤„ë§í•©ë‹ˆë‹¤. - DEPRECATED
 * ì´ì œ ëŒ€í™” í…ìŠ¤íŠ¸ê°€ 5ê°œì”© ìŒ“ì¼ ë•Œë§ˆë‹¤ ë¶„ì„ì´ íŠ¸ë¦¬ê±°ë©ë‹ˆë‹¤.
 */
function startAnalysisInterval() {
    // âœ… ë³€ê²½: ì´ì œ ë¶„ì„ì€ ëŒ€í™” í…ìŠ¤íŠ¸ê°€ 5ê°œì”© ìŒ“ì¼ ë•Œë§ˆë‹¤ triggerAnalysisIfNeeded()ì—ì„œ íŠ¸ë¦¬ê±°ë©ë‹ˆë‹¤.
    console.log('â° Analysis will be triggered every 5 conversation texts (not on timer)');
    
    // ê¸°ì¡´ ì¸í„°ë²Œ ì •ë¦¬ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    if (analysisIntervalId) {
        clearInterval(analysisIntervalId);
        analysisIntervalId = null;
    }
}

function stopAnalysisInterval() {
    if (analysisIntervalId) {
        clearInterval(analysisIntervalId);
        analysisIntervalId = null;
    }
    
    // ë°±ì—… ë©”ì»¤ë‹ˆì¦˜ íƒ€ì´ë¨¸ë“¤ë„ ì •ë¦¬
    if (myInactivityTimer) {
        clearTimeout(myInactivityTimer);
        myInactivityTimer = null;
    }
    if (theirInactivityTimer) {
        clearTimeout(theirInactivityTimer);
        theirInactivityTimer = null;
    }
}

function sendToRenderer(channel, data) {
    BrowserWindow.getAllWindows().forEach(win => {
        if (!win.isDestroyed()) {
            win.webContents.send(channel, data);
        }
    });
}

// âœ… ì¶”ê°€: getCurrentSessionData í•¨ìˆ˜ ì •ì˜
function getCurrentSessionData() {
    return {
        sessionId: currentSessionId,
        conversationHistory: conversationHistory, // ì´ì œ í…ìŠ¤íŠ¸ ë°°ì—´
        totalTexts: conversationHistory.length
    };
}

// Conversation management functions
async function initializeNewSession() {
    try {
        const uid = dataService.currentUserId; // Get current user (local or firebase)
        currentSessionId = await sqliteClient.createSession(uid);
        console.log(`[DB] New session started in DB: ${currentSessionId}`);
        
        conversationHistory = [];
        myCurrentUtterance = '';
        theirCurrentUtterance = '';
        
        // sendToRenderer('update-outline', []);
        // sendToRenderer('update-analysis-requests', []);

            // ë°±ì—… ë©”ì»¤ë‹ˆì¦˜ ìƒíƒœ ë¦¬ì…‹
    myLastPartialText = '';
    theirLastPartialText = '';
    if (myInactivityTimer) {
        clearTimeout(myInactivityTimer);
        myInactivityTimer = null;
    }
    if (theirInactivityTimer) {
        clearTimeout(theirInactivityTimer);
        theirInactivityTimer = null;
    }
    
    console.log('New conversation session started:', currentSessionId);
        return true;
    } catch (error) {
        console.error("Failed to initialize new session in DB:", error);
        currentSessionId = null;
        return false;
    }
}



async function saveConversationTurn(speaker, transcription) {
    if (!currentSessionId) {
        console.log("No active session, initializing a new one first.");
        const success = await initializeNewSession();
        if (!success) {
            console.error("Could not save turn because session initialization failed.");
            return;
        }
    }
    if (transcription.trim() === '') return;

    try {
        await sqliteClient.addTranscript({
            sessionId: currentSessionId,
            speaker: speaker, // 'Me' or 'Them'
            text: transcription.trim(),
        });
        console.log(`[DB] Saved transcript for session ${currentSessionId}: (${speaker})`);

        // For prompt building, we might still want to use a temporary history
        const conversationText = `${speaker.toLowerCase()}: ${transcription.trim()}`;
        conversationHistory.push(conversationText);
        console.log(`ğŸ’¬ Saved conversation text: ${conversationText}`);
        console.log(`ğŸ“ˆ Total conversation history: ${conversationHistory.length} texts`);

        // âœ… ë³€ê²½: 5ê°œì”© ìŒ“ì¼ ë•Œë§ˆë‹¤ ë¶„ì„ íŠ¸ë¦¬ê±°
        triggerAnalysisIfNeeded();

        // Send to renderer for live view
        const conversationTurn = {
            speaker: speaker,
            timestamp: Date.now(),
            transcription: transcription.trim(),
        };
        sendToRenderer('update-live-transcription', { turn: conversationTurn });
        // ğŸ“ 5í„´ë§ˆë‹¤ ë˜ëŠ” ì¤‘ìš”í•œ ëŒ€í™”ì¼ ë•Œ ìë™ ì €ì¥
        if (conversationHistory.length % 5 === 0) {
            console.log(`ğŸ”„ Auto-saving conversation session ${currentSessionId} (${conversationHistory.length} turns)`);
            sendToRenderer('save-conversation-session', {
                sessionId: currentSessionId,
                conversationHistory: conversationHistory

            });
        }
    } catch (error) {
        console.error("Failed to save transcript to DB:", error);
    }
}


async function initializeLiveSummarySession(language = 'en') {
    if (isInitializingSession) {
        console.log('Session initialization already in progress.');
        return false;
    }

    const loggedIn = isFirebaseLoggedIn(); 
    const keyType  = loggedIn ? 'vKey' : 'apiKey';

    isInitializingSession = true;
    sendToRenderer('session-initializing', true);
    sendToRenderer('update-status', 'Initializing sessions...');

    // Merged block
    const API_KEY = getApiKey();
    if (!API_KEY) {
        console.error('FATAL ERROR: API Key is not defined.');
        sendToRenderer('update-status', 'API Key not configured.');
        isInitializingSession = false;
        sendToRenderer('session-initializing', false);
        return false;
    }

    initializeNewSession();

    try {
        // 1. ì‚¬ìš©ì(ë§ˆì´í¬) STT ì„¸ì…˜ ì½œë°± ì •ì˜ (ìˆ˜ì • ì™„ë£Œ)
        const handleMyMessage = message => {
            const type = message.type;
            const text = message.transcript || message.delta ||(message.alternatives && message.alternatives[0]?.transcript) || '';
            // console.log('ğŸ¤ handleMyMessage', { type, message });

            if (type === 'conversation.item.input_audio_transcription.delta') {
                // New delta proves the speaker is still talking.
                // Cancel any pending completion flush to avoid premature cuts.
                if (myCompletionTimer) {
                    // console.log('ğŸ¤ Delta received, canceling pending completion flush for "Me"');
                    clearTimeout(myCompletionTimer);
                    myCompletionTimer = null;
                }

                // Accumulate deltas for the current utterance.
                myCurrentUtterance += text;

                // For the UI, show the buffered text plus the new delta sequence so it looks continuous.
                const continuousText = myCompletionBuffer + (myCompletionBuffer ? ' ' : '') + myCurrentUtterance;

                // Realtime partial update âœ renderer (still streaming)
                if (text && !text.includes('vq_lbr_audio_')) {
                    sendToRenderer('stt-update', {
                        speaker: 'Me',
                        text: continuousText,
                        isPartial: true,
                        isFinal: false,
                        timestamp: Date.now(),
                    });
                }
            } else if (type === 'conversation.item.input_audio_transcription.completed') {
                if (text && text.trim()) {
                    // A 'completed' event provides the full, final text for an utterance.
                    // We discard any intermediate deltas for this segment and use this text.
                    const finalUtteranceText = text.trim();
                    myCurrentUtterance = ''; // Reset the delta accumulator.

                    // Debounce this whole utterance to merge quick successive utterances.
                    debounceMyCompletion(finalUtteranceText);
                }
            } else if (message.error) {
                console.error('[Me] STT Session Error:', message.error);
            }
        };

        // 2. ìƒëŒ€ë°©(ì‹œìŠ¤í…œ ì˜¤ë””ì˜¤) STT ì„¸ì…˜ ì½œë°± ì •ì˜ (ìˆ˜ì • ì™„ë£Œ)
        const handleTheirMessage = message => {
            const type = message.type;
            // console.log('ğŸ”¥ handleTheirMessage', { type, message });
            const text = message.transcript || message.delta ||(message.alternatives && message.alternatives[0]?.transcript) || '';

            if (type === 'conversation.item.input_audio_transcription.delta') {
                // New delta proves the speaker is still talking.
                // Cancel any pending completion flush to avoid premature cuts.
                if (theirCompletionTimer) {
                    // console.log('ğŸ”¥ Delta received, canceling pending completion flush for "Them"');
                    clearTimeout(theirCompletionTimer);
                    theirCompletionTimer = null;
                }
                
                // Accumulate deltas for the current utterance.
                theirCurrentUtterance += text;

                // For the UI, show the buffered text plus the new delta sequence so it looks continuous.
                const continuousText = theirCompletionBuffer + (theirCompletionBuffer ? ' ' : '') + theirCurrentUtterance;

                if (text && !text.includes('vq_lbr_audio_')) {
                    sendToRenderer('stt-update', {
                        speaker: 'Them',
                        text: continuousText,
                        isPartial: true,
                        isFinal: false,
                        timestamp: Date.now(),
                    });
                }
            } else if (type === 'conversation.item.input_audio_transcription.completed') {
                if (text && text.trim()) {
                    // A 'completed' event provides the full, final text for an utterance.
                    // We discard any intermediate deltas for this segment and use this text.
                    const finalUtteranceText = text.trim();
                    theirCurrentUtterance = ''; // Reset the delta accumulator.

                    // Debounce this whole utterance to merge quick successive utterances.
                    debounceTheirCompletion(finalUtteranceText);
                }
            } else if (message.error) {
                console.error('[Them] STT Session Error:', message.error);
            }
        };

        // STT ì„¸ì…˜ ì„¤ì • ê°ì²´
        const mySttConfig = {
            language: language,
            callbacks: {
                onmessage: handleMyMessage,
                onerror: (error) => console.error('My STT session error:', error.message),
                onclose: (event) => console.log('My STT session closed:', event.reason)
            }
        };
        const theirSttConfig = {
            language: language,
            callbacks: {
                onmessage: handleTheirMessage,
                onerror: (error) => console.error('Their STT session error:', error.message),
                onclose: (event) => console.log('Their STT session closed:', event.reason)
            }
        };

        [mySttSession, theirSttSession] = await Promise.all([
            connectToOpenAiSession(API_KEY, mySttConfig, keyType),
            connectToOpenAiSession(API_KEY, theirSttConfig, keyType),
        ]);

        console.log("âœ… Both STT sessions initialized successfully.");
        // startAnalysisInterval();
        triggerAnalysisIfNeeded();

        sendToRenderer('session-state-changed', { isActive: true });

        isInitializingSession = false;
        sendToRenderer('session-initializing', false);
        sendToRenderer('update-status', 'Connected. Ready to listen.');
        return true;

    } catch (error) {
        console.error('âŒ Failed to initialize OpenAI STT sessions:', error);
        isInitializingSession = false;
        sendToRenderer('session-initializing', false);
        sendToRenderer('update-status', 'Initialization failed.');
        mySttSession = null;
        theirSttSession = null;
        return false;
    }
}

function killExistingSystemAudioDump() {
    return new Promise(resolve => {
        console.log('Checking for existing SystemAudioDump processes...');

        // Kill any existing SystemAudioDump processes
        const killProc = spawn('pkill', ['-f', 'SystemAudioDump'], {
            stdio: 'ignore',
        });

        killProc.on('close', code => {
            if (code === 0) {
                console.log('Killed existing SystemAudioDump processes');
            } else {
                console.log('No existing SystemAudioDump processes found');
            }
            resolve();
        });

        killProc.on('error', err => {
            console.log('Error checking for existing processes (this is normal):', err.message);
            resolve();
        });

        // Timeout after 2 seconds
        setTimeout(() => {
            killProc.kill();
            resolve();
        }, 2000);
    });
}

async function startMacOSAudioCapture() {
    if (process.platform !== 'darwin' || !theirSttSession) return false;

    await killExistingSystemAudioDump();
    console.log('Starting macOS audio capture for "Them"...');

    const { app } = require('electron');
    const path = require('path');
    let systemAudioPath = app.isPackaged
        ? path.join(process.resourcesPath, 'SystemAudioDump')
        : path.join(__dirname, '../../assets', 'SystemAudioDump');

    console.log('SystemAudioDump path:', systemAudioPath);

    systemAudioProc = spawn(systemAudioPath, [], {
        stdio: ['ignore', 'pipe', 'pipe'],
    });

    if (!systemAudioProc.pid) {
        console.error('Failed to start SystemAudioDump');
        return false;
    }

    console.log('SystemAudioDump started with PID:', systemAudioProc.pid);

    const CHUNK_DURATION = 0.1; // 500ms -> 100msë¡œ ë³µì›í•˜ì—¬ ë” ë¹ ë¥¸ ë°˜ì‘ì„± ì¶”êµ¬
    const SAMPLE_RATE = 24000;
    const BYTES_PER_SAMPLE = 2;
    const CHANNELS = 2;
    const CHUNK_SIZE = SAMPLE_RATE * BYTES_PER_SAMPLE * CHANNELS * CHUNK_DURATION;

    let audioBuffer = Buffer.alloc(0);

    // â­ï¸ [ìˆ˜ì • 1] ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ë¥¼ async í•¨ìˆ˜ë¡œ ë³€ê²½
    systemAudioProc.stdout.on('data', async data => {
        audioBuffer = Buffer.concat([audioBuffer, data]);
        // console.log(`System audio data received, buffer length: ${audioBuffer.length}`); // DEBUG

        while (audioBuffer.length >= CHUNK_SIZE) {
            const chunk = audioBuffer.slice(0, CHUNK_SIZE);
            audioBuffer = audioBuffer.slice(CHUNK_SIZE);

            const monoChunk = CHANNELS === 2 ? convertStereoToMono(chunk) : chunk;
            const base64Data = monoChunk.toString('base64');

            sendToRenderer('system-audio-data', { data: base64Data });

            if (theirSttSession) {
                // â­ï¸ [ìˆ˜ì • 2] try...catchì™€ await ì‚¬ìš©
                try {
                    // console.log('Sending system audio chunk to OpenAI...'); // DEBUG
                    // await theirSttSession.sendRealtimeInput({
                    //     audio: { data: base64Data, mimeType: 'audio/pcm;rate=24000' },
                    // });
                    await theirSttSession.sendRealtimeInput(base64Data);
                } catch (err) {
                    console.error('Error sending system audio:', err.message);
                }
            }

            if (process.env.DEBUG_AUDIO) {
                saveDebugAudio(monoChunk, 'system_audio');
            }
        }
    });

    systemAudioProc.stderr.on('data', data => {
        console.error('SystemAudioDump stderr:', data.toString());
    });

    systemAudioProc.on('close', code => {
        console.log('SystemAudioDump process closed with code:', code);
        systemAudioProc = null;
    });

    systemAudioProc.on('error', err => {
        console.error('SystemAudioDump process error:', err);
        systemAudioProc = null;
    });

    return true;
}

function convertStereoToMono(stereoBuffer) {
    const samples = stereoBuffer.length / 4;
    const monoBuffer = Buffer.alloc(samples * 2);

    for (let i = 0; i < samples; i++) {
        const leftSample = stereoBuffer.readInt16LE(i * 4);
        monoBuffer.writeInt16LE(leftSample, i * 2);
    }

    return monoBuffer;
}

function stopMacOSAudioCapture() {
    if (systemAudioProc) {
        console.log('Stopping SystemAudioDump...');
        systemAudioProc.kill('SIGTERM');
        systemAudioProc = null;
    }
}

async function sendAudioToOpenAI(base64Data, sttSessionRef) {
    if (!sttSessionRef.current) return;

    try {
        process.stdout.write('.');
        await sttSessionRef.current.sendRealtimeInput({
            audio: {
                data: base64Data,
                mimeType: 'audio/pcm;rate=24000',
            },
        });
    } catch (error) {
        console.error('Error sending audio to OpenAI:', error);
    }
}

function isSessionActive() {
    return !!mySttSession && !!theirSttSession;
}

async function closeSession() {
    try {
        stopMacOSAudioCapture();
        stopAnalysisInterval(); // ë¶„ì„ ì¸í„°ë²Œ ì¤‘ì§€

        if (currentSessionId) {
            await sqliteClient.endSession(currentSessionId);
            console.log(`[DB] Session ${currentSessionId} ended.`);
        }

        const closePromises = [];
        if (mySttSession) {
            closePromises.push(mySttSession.close());
            mySttSession = null;
        }
        if (theirSttSession) {
            closePromises.push(theirSttSession.close());
            theirSttSession = null;
        }

        await Promise.all(closePromises);
        console.log('All sessions closed.');
        
        // ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        currentSessionId = null;
        conversationHistory = [];

        sendToRenderer('session-state-changed', { isActive: false });
        sendToRenderer('session-did-close'); // Notify manager to hide window

        return { success: true };
    } catch (error) {
        console.error('Error closing sessions:', error);
        return { success: false, error: error.message };
    }
}

function setupLiveSummaryIpcHandlers() {
    // New handler to check session status
    ipcMain.handle('is-session-active', async () => {
        // A session is considered active if the STT session objects exist.
        const isActive = isSessionActive();
        console.log(`Checking session status. Active: ${isActive}`);
        return isActive;
    });

    ipcMain.handle('initialize-openai', async (event, profile = 'interview', language = 'en') => {
        // The API key from .env is used within initializeLiveSummarySession.
        console.log(`Received initialize-openai request with profile: ${profile}, language: ${language}`);
        const success = await initializeLiveSummarySession();
        return success;
    });

    // send-audio-content í•¸ë“¤ëŸ¬: ì‚¬ìš©ì ë§ˆì´í¬ ì˜¤ë””ì˜¤ë¥¼ 'mySttSession'ìœ¼ë¡œ ì „ì†¡
    ipcMain.handle('send-audio-content', async (event, { data, mimeType }) => {
        if (!mySttSession) return { success: false, error: 'User STT session not active' };
        try {
            // console.log('Received mic audio data from renderer.'); // DEBUG
            // process.stdout.write('M'); // 'M' for My audio
            // await mySttSession.sendRealtimeInput({
            //     audio: { data: data, mimeType: mimeType },
            // });
            await mySttSession.sendRealtimeInput(data);
            return { success: true };
        } catch (error) {
            console.error('Error sending user audio:', error);
            return { success: false, error: error.message };
        }
    });

    // start-macos-audio í•¸ë“¤ëŸ¬: ì‹œìŠ¤í…œ ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œì‘ (ë‚´ë¶€ì ìœ¼ë¡œ 'theirSttSession' ì‚¬ìš©)
    ipcMain.handle('start-macos-audio', async () => {
        if (process.platform !== 'darwin') {
            return { success: false, error: 'macOS audio capture only available on macOS' };
        }
        try {
            const success = await startMacOSAudioCapture();
            return { success };
        } catch (error) {
            console.error('Error starting macOS audio capture:', error);
            return { success: false, error: error.message };
        }
    });

    // â­ï¸ [ìˆ˜ì • 2] ëˆ„ë½ëœ stop-macos-audio í•¸ë“¤ëŸ¬ ì¶”ê°€
    ipcMain.handle('stop-macos-audio', async () => {
        try {
            stopMacOSAudioCapture();
            return { success: true };
        } catch (error) {
            console.error('Error stopping macOS audio capture:', error);
            return { success: false, error: error.message };
        }
    });


    // í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ë° ë¶„ì„ ìš”ì²­ì€ ì´ì œ renderer.jsì—ì„œ ì²˜ë¦¬ë¨
    // renderer.jsì—ì„œ ìƒˆë¡œìš´ chatModelì„ ìƒì„±í•˜ê³  ëŒ€í™”ë‚´ì—­ + ìŠ¤í¬ë¦°ìƒ·ê³¼ í•¨ê»˜ ì „ì†¡

    // ëŒ€í™” ê¸°ë¡ì„ rendererë¡œ ì œê³µí•˜ëŠ” í•¸ë“¤ëŸ¬
    ipcMain.handle('get-conversation-history', async () => {
        try {
            const formattedHistory = formatConversationForPrompt(conversationHistory);
            console.log(`ğŸ“¤ Sending conversation history to renderer: ${conversationHistory.length} texts`);
            return { success: true, data: formattedHistory };
        } catch (error) {
            console.error('Error getting conversation history:', error);
            return { success: false, error: error.message };
        }
    });

    ipcMain.handle('close-session', async () => {
        return await closeSession();
    });

    // Conversation history IPC handlers
    ipcMain.handle('get-current-session', async event => {
        try {
            return { success: true, data: getCurrentSessionData() };
        } catch (error) {
            console.error('Error getting current session:', error);
            return { success: false, error: error.message };
        }
    });

    ipcMain.handle('start-new-session', async event => {
        try {
            initializeNewSession();
            return { success: true, sessionId: currentSessionId };
        } catch (error) {
            console.error('Error starting new session:', error);
            return { success: false, error: error.message };
        }
    });

    ipcMain.handle('update-google-search-setting', async (event, enabled) => {
        try {
            console.log('Google Search setting updated to:', enabled);
            // The setting is already saved in localStorage by the renderer
            // This is just for logging/confirmation
            return { success: true };
        } catch (error) {
            console.error('Error updating Google Search setting:', error);
            return { success: false, error: error.message };
        }
    });
}

module.exports = {
    initializeLiveSummarySession,
    sendToRenderer,
    initializeNewSession,
    saveConversationTurn,
    killExistingSystemAudioDump,
    startMacOSAudioCapture,
    convertStereoToMono,
    stopMacOSAudioCapture,
    sendAudioToOpenAI,
    setupLiveSummaryIpcHandlers,
    isSessionActive,
    closeSession,
};
